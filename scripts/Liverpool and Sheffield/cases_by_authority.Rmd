---
title: "cases_by_council"
author: "George WIlloughby"
date: "02/06/2021"
output: html_document
---
```{r}
library(jsonlite)
library(httr)
library(dplyr)
```

# Get the latest case numbers from the lower tier local authorities

```{r}
#' Extracts paginated data by requesting all of the pages
#' and combining the results.
#'
#' @param filters    API filters. See the API documentations for 
#'                   additional information.
#'                   
#' @param structure  Structure parameter. See the API documentations 
#'                   for additional information.
#'                   
#' @return list      Comprehensive list of dictionaries containing all 
#'                   the data for the given ``filter`` and ``structure`.`
get_paginated_data <- function (filters, structure) {
  
    endpoint     <- "https://api.coronavirus.data.gov.uk/v1/data"
    results      <- list()
    current_page <- 1
    
    repeat {

        httr::GET(
            url   = endpoint,
            query = list(
                filters   = paste(filters, collapse = ";"),
                structure = jsonlite::toJSON(structure, auto_unbox = TRUE),
                page      = current_page
            ),
            timeout(10)
        ) -> response
        
        # Handle errors:
        if ( response$status_code >= 400 ) {
            err_msg = httr::http_status(response)
            stop(err_msg)
        } else if ( response$status_code == 204 ) {
            break
        }
        
        # Convert response from binary to JSON:
        json_text <- content(response, "text")
        dt        <- jsonlite::fromJSON(json_text)
        results   <- rbind(results, dt$data)
        
        if ( is.null( dt$pagination$`next` ) ){
            break
        }
        
        current_page <- current_page + 1;

    }
    
    return(results)
    
}


# Create filters:
query_filters <- c(
    "areaType=ltla"
)

# Create the structure as a list or a list of lists:
query_structure <- list(
    date       = "date", 
    name       = "areaName", 
    code       = "areaCode", 
    daily      = "newCasesBySpecimenDate",
    cumulative = "cumCasesBySpecimenDate"
)

latest_lower_authority <- get_paginated_data(query_filters, query_structure)

list(
  "Shape"                = dim(latest_lower_authority),
  "Data (first 3 items)" = latest_lower_authority[0:3, 0:-1]
) -> report

print(report)
```

# Get the latest case numbers from the upper tier local authorities

```{r}
#' Extracts paginated data by requesting all of the pages
#' and combining the results.
#'
#' @param filters    API filters. See the API documentations for 
#'                   additional information.
#'                   
#' @param structure  Structure parameter. See the API documentations 
#'                   for additional information.
#'                   
#' @return list      Comprehensive list of dictionaries containing all 
#'                   the data for the given ``filter`` and ``structure`.`
get_paginated_data <- function (filters, structure) {
  
    endpoint     <- "https://api.coronavirus.data.gov.uk/v1/data"
    results      <- list()
    current_page <- 1
    
    repeat {

        httr::GET(
            url   = endpoint,
            query = list(
                filters   = paste(filters, collapse = ";"),
                structure = jsonlite::toJSON(structure, auto_unbox = TRUE),
                page      = current_page
            ),
            timeout(10)
        ) -> response
        
        # Handle errors:
        if ( response$status_code >= 400 ) {
            err_msg = httr::http_status(response)
            stop(err_msg)
        } else if ( response$status_code == 204 ) {
            break
        }
        
        # Convert response from binary to JSON:
        json_text <- content(response, "text")
        dt        <- jsonlite::fromJSON(json_text)
        results   <- rbind(results, dt$data)
        
        if ( is.null( dt$pagination$`next` ) ){
            break
        }
        
        current_page <- current_page + 1;

    }
    
    return(results)
    
}


# Create filters:
query_filters <- c(
    "areaType=utla"
)

# Create the structure as a list or a list of lists:
query_structure <- list(
    date       = "date", 
    name       = "areaName", 
    code       = "areaCode", 
    daily      = "newCasesBySpecimenDate",
    cumulative = "cumCasesBySpecimenDate"
)

latest_upper_authority <- get_paginated_data(query_filters, query_structure)

list(
  "Shape"                = dim(latest_upper_authority),
  "Data (first 3 items)" = latest_upper_authority[0:3, 0:-1]
) -> report

print(report)

```

# Merging the lower and upper tier authorities together

```{r}
all_authorities <- rbind(latest_lower_authority, latest_upper_authority)
```

# Renaming the columns

```{r}
all_authorities <- all_authorities %>%
  rename(
    daily_cases = daily,
    cumulative_cases = cumulative
  )

all_authorities
```


# Removing duplicates from the new dataset

```{r}
library(tidyverse)

all_authorities <- distinct(all_authorities)

all_authorities
```


# Using regex to remove the local authorities from Scotland and Wales

We can use regex to include all the data we need just for England by using [E]. It asks if a word starts with the letter E and it will create another column for when it does. In this case, it will get the cases for the authorities in England.

```{r}
all_authorities$England <- grepl("[E]", all_authorities$code)
#Remove LAs in Wales and Scotland
all_authorities <- subset(all_authorities, all_authorities$England == TRUE)

head(all_authorities)
```

# Calculating the rate per 100k for the local authorities in England

To do this, we will need to import the latest population figures for all of the authorities. Data for this is supplied by the Office for National Statistics and we can import it.

```{r}
#Importing the population figures

populationestimates <- read.csv("updatedestimations.csv")

populationestimates <- populationestimates %>%
  rename(
    name = Name
  )

populationestimates

#Merging the population figures with the 'all_authorities' data

all_authorities_population <- merge(all_authorities, populationestimates, by = "name")

all_authorities_population
```

# Dropping duplicate columns from the updated dataset

From the new updated dataset, you should be able to see we have some duplicated columns. For example, code is repeated. We are also only looking for the overall population for each authority so we can remove all the age brackets that procede the 'All.ages. column.

```{r}
#Removing the columns we don't need
all_authorities_population <- all_authorities_population [-c(6,7, 10:100)]

all_authorities_population

#Renaming columns to make them clearer 
library(dplyr)

all_authorities_population <- all_authorities_population %>%
  rename(
    population = All.ages
  )
```

# Calculating the case rates per 100k

To do this, we need to do some multiplication.

```{r}
#Calculating the rate per 100k
all_authorities_population$case_rate <- (all_authorities_population$daily_cases/all_authorities_population$population) *100000

all_authorities_population
```

# Figuring out the weekly sum 

The next thing to calculate is the rolling sum of cases for each authority. We can then use the arrange() function in the dplyr() library to put the authorities in descending order from largest to smallest in terms of case rates.

```{r}
#Activate package and calculate the sum of case rates
library(dplyr)
library(zoo)

all_authorities_population_sum <- all_authorities_population %>%
  dplyr::arrange(desc(date)) %>%
  dplyr::group_by(name) %>%
  dplyr::mutate(cases_seven_day_average = zoo::rollsum(case_rate, k = 7, align="left", fill = NA)) %>%
  dplyr::ungroup()

view(all_authorities_population_sum)
```

# Exporting the data

```{r}
write.csv(all_authorities_population_sum, "case_rates_england_authorities.csv")
```








